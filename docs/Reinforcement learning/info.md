# Reinforcement Learning / Обучение с подкреплением

environment, e - сценарий/окружение, с которым взаимодействует агент.

Agent - представляет собой программу или систему, которая выполняет действия в среде, чтобы получить некоторую награду.

action, a - действие агента в среде

reward, R - представляет собой числовую оценку или сигнал, который агент получает от среды за выполнение определенного действия в определенном состоянии. Награда может быть положительной, отрицательной или нулевой.

state, s - это описание текущего положения среды.

* Полная наблюдаемость (full observability), которая гласит, что агент в своей стратегии наблюдает всё состояние \( s \) полностью и может выбирать действия, зная об окружающем мире абсолютно всё
* Частично наблюдаемых (Partially observable), которая гласит, что агент в своей стратегии наблюдает только часть состояние \( s \)

эпизод (episode) - одна итерация взаимодействия от начального состояния до попадания в терминальное состояние

Цепочка генерируемых в ходе взаимодействия случайных величин \( s_0, a_0, s_1, a_1, s_2, a_2, \dots \) называется траекторией (trajectory)

policy function, p - стратегия, которая применяется агентом для принятия решения о следующем действии на основе текущего состояния.

* Детерминированная (deterministic policy): \( a = p(s) \)
* Стохастическая (stochastic policy): \( p(a|s) \)

value function, V - выдает вероятность награды исходя из state \( v(s) \)

оптимальная Q^* function, Q \( Q(s, a) \) это то, сколько максимально награды можно (в среднем) набрать после выбора действия \( a\) из состояния \(s\).

$$
Q^{*}(s, a) = \max_{\pi} \mathbb{E}_{\mathcal{T} \sim \pi \mid s_0 = s, a_0 = a} \sum_{t \ge 0} \gamma^t r_t
$$

\(Q^{*}(s, a)\), нужно перебрать все стратегии, посмотреть, сколько каждая из них набирает награды после выбора $a$ из состояния $s$, и взять наилучшую стратегию.

Выбор того действия, на котором достигается максимум по действиям Q-функции, называется жадным (greedy) по отношению к ней. 